# GEMINI.md - Project Overview

## Project Overview

This project is an interactive learning platform designed to teach Java programming. It consists of a web-based frontend built with React and a Python backend powered by FastAPI.

The platform provides several key features:

*   **AI-Powered Tutoring:** A sophisticated RAG (Retrieval-Augmented Generation) pipeline that uses multiple large language models (GPT, Gemini, DeepSeek) to answer student questions. The system leverages content from PDF lecture notes and a JSON-based knowledge base.
*   **Interactive Lessons:** Lessons are generated from PDF documents and presented to the user in a structured format.
*   **Practical Code Evaluation:** Students can submit Java code to solve practical problems. The code is evaluated for correctness using the Paiza API.
*   **Java Code Execution:** The platform can compile and run arbitrary Java code, providing immediate feedback to the user.
*   **Syntax Checking:** The system can check the syntax of Java code and highlight errors.

The backend uses a PostgreSQL database to cache results from the RAG pipeline, improving performance and reducing redundant API calls.

## Building and Running

### Backend (FastAPI)

1.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

2.  **Set up Environment Variables:**
    The application requires API keys for various services. Create a `.env` file in the root directory and add the following variables:

    ```
    GENAI_API_key="your-genai-hkbu-api-key"
    PAIZA_API_KEY="your-paiza-api-key"
    ```

3.  **Database Setup:**
    The application uses a PostgreSQL database. Make sure you have a running PostgreSQL server. The connection string is defined in `database.py`. You may need to create the database `fypdb` and user `hei`.

4.  **Run the Application:**
    ```bash
    uvicorn asking_ai:app --reload
    ```
    The backend will be available at `http://localhost:8000`.

### Frontend (React)

1.  **Navigate to the frontend directory:**
    ```bash
    cd explain-frontend
    ```

2.  **Install Dependencies:**
    ```bash
    npm install
    ```

3.  **Run the Application:**
    ```bash
    npm start
    ```
    The frontend will be available at `http://localhost:3000` and will connect to the backend API.

## Development Conventions

*   **Backend:** The backend is a FastAPI application. The main entry point is `asking_ai.py`.
    *   **Database:** A PostgreSQL database is used for caching. The database model is defined in `models.py`.
    *   **Data Preprocessing:** The `tutor.py` script is used to process PDF lecture notes into a JSON format that can be used by the application.
*   **Frontend:** The frontend is a React application created with `create-react-app`.
    *   **Components:** The main components are in the `src` directory.
*   **Content:**
    *   `lessons_raw/`: Contains the raw JSON data for the lessons.
    *   `practical_tests/`: Contains the practical test questions and solutions.
    *   `frontend/Lecture Notes-20250622/`: Contains the PDF lecture notes.
*   **AI:** The RAG pipeline in `asking_ai.py` is the core of the AI tutoring system. It uses a multi-step process of routing, context retrieval, answer generation, verification, and arbitration.


## Detail of RAG
1.1 Gather and Clean Domain Documents

Collect your domain corpus (e.g., textbooks, articles, relevant documentation).
Clean and segment documents into paragraphs or logical chunks.
Remove duplicates, normalize text, and ensure consistent tokenization.
1.2 Indexing and Document Embeddings

Build a single Sparse Encoder index (e.g., ELSER-style) using the “Best Fields” hybrid query strategy (metadata, title, context).
Generate sparse embeddings for each document chunk and index them in a vector database (e.g., FAISS with sparse encoder support).
Omit dense vector and BM25 indices to reduce complexity and resource use.
1.3 Dual Retrieval Preparation (Main Problem + Subproblems)

Prepare your indexing and retrieval system to accept two types of queries: one for the main problem and one for subproblems generated by problem decomposition.
Use the same sparse encoder index for both query types.
This supports fine-grained retrieval aligned with tutoring scaffolding.
─────────────

DATASET CREATION FOR TRAINING
2.1 SELF-RAG-Style Reflection/On-Demand Retrieval Data

Generate or gather synthetic examples where the model learns to decide whether to retrieve (`Retrieve=Yes/No`).
Annotate training outputs with simplified reflection tokens:
`ISREL` (Relevant/Irrelevant) for passage relevance.
`ISSUP` (Fully/Partially/NoSupport) for how well the passage supports the output.
Keep the reflection token set minimal to reduce training and inference overhead.
2.2 Scaffolding Dataset (CLASS Approach)

Create domain-specific challenging problems broken into subproblems.
For each subproblem, provide:
Correct answer
Hint
Example incorrect student response
Feedback/correction to incorrect response
This dataset teaches stepwise problem-solving and error correction.
2.3 Conversational Dataset (CLASS Approach)

Simulate tutor-student dialogues focusing on:
Stepwise subproblem solving
Handling correct, partially correct, incorrect, and off-topic student responses
Use a simple structured response template with key fields:
`Decision by Tutorbot` (e.g., correct, hint, off-topic)
`Subproblem` (current focus)
`Tutorbot Response` (hint/explanation)
Avoid complex multi-action decision coding to simplify training.
2.4 Optional Blended RAG Metadata

Since only sparse encoder retrieval is used, you can omit labeling passages by retrieval method.
─────────────────────────────────────────────────────────────────

MODEL ARCHITECTURE AND TRAINING
3.1 Base Model Selection

Use a resource-efficient open-source LM such as Vicuna-7B or Llama2-7B.
Ensure support for instruction tuning and token-level control to generate reflection tokens.
3.2 Pre-Training (Domain Fine-Tuning)

Fine-tune the base LM on your domain corpus using next-token prediction to build domain knowledge.
3.3 Instruction Tuning with Your Synthetic Datasets

Merge SELF-RAG reflection data, CLASS scaffolding, and conversational datasets into a unified instruction-tuning dataset.
Train the model to generate:
User-facing tutorbot responses
Internal reflection tokens (`Retrieve=Yes/No`, `ISREL`, `ISSUP`)
Structured fields such as `Decision by Tutorbot` and `Subproblem`
Use cross-entropy loss with masking for reflection tokens to ensure consistent generation.
3.4 Verification and Early Tests

Hold out some synthetic or real user dialogues for validation.
Test the model’s ability to:
Produce accurate reflection tokens
Make adaptive retrieval decisions
Handle partial correctness and provide hints.
─────────────────────────────────────────────────────────────────

INFERENCE-TIME PIPELINE: BLENDED RETRIEVAL + SELF-RAG + CLASS
4.1 Conversation Manager

Maintain conversation history and track the current subproblem.
Manage transitions between subproblems and overall dialogue flow.
4.2 Adaptive Retrieval Decision (SELF-RAG)

For each user input or model turn, generate `Retrieve=Yes/No` token to decide if retrieval is necessary.
If `Retrieve=Yes`, trigger retrieval for the current subproblem or main problem.
4.3 Sparse Encoder Retrieval

Retrieve top-k passages from the sparse encoder index using Best Fields queries.
Use only this single retrieval source to minimize complexity.
4.4 Reflection & Critique of Retrieved Passages (SELF-RAG)

Generate reflection tokens assessing passage relevance (`ISREL`) and supportiveness (`ISSUP`).
Use these tokens internally to filter or weigh passages for generation.
Optionally discard irrelevant or unsupported passages.
4.5 Tutor-Like Response (CLASS)

Generate structured responses containing:
`Decision by Tutorbot` (feedback type)
`Subproblem` label (current or next focus)
Tutorbot’s natural language explanation or hint
Show only the tutorbot’s message to the user; keep reflection tokens hidden.
4.6 User Interaction

Support user requests for hints, solutions, or navigation between subproblems.
Tutorbot responds according to the structured template.
─────────────────────────────────────────────────────────────────

ITERATIVE REFINEMENT AND CONTINUOUS LEARNING
5.1 Logging and Feedback

Log conversations, noting points of user confusion or dissatisfaction.
Track retrieval decisions and reflection token correctness.
5.2 Error Analysis on Reflection Tokens and Critique

Analyze mismatches between reflection tokens and actual passage correctness.
Identify missed or unnecessary retrieval calls.
5.3 Additional Fine-Tuning with User Data

Incorporate real user dialogues into scaffolding and conversational datasets.
Periodically fine-tune the model to improve behavior.
5.4 Expanding to More Complex Queries

Optionally add modules for math/code reasoning or multimodal inputs as needed.
Keep these extensions modular to maintain system efficiency.
─────────────────────────────────────────────────────────────────

SYSTEM DEPLOYMENT AND MAINTENANCE
6.1 Serving the Model

Deploy the fine-tuned LM behind inference endpoints (e.g., vLLM, Hugging Face Inference API).
Integrate conversation manager and retrieval pipeline into a microservice or serverless architecture.
6.2 Monitoring

Track usage metrics: conversation length, retrieval latency, reflection token usage.
Monitor frequency of hints and feedback responses.
6.3 Versioning

Maintain model versions and conduct A/B testing for new releases.
6.4 Ethical and Privacy Concerns

Ensure compliance with data privacy regulations for tutoring conversations.
Provide disclaimers about AI limitations and encourage users to verify critical facts.
─────────────────────────────────────────────────────────────────

SUMMARY
By focusing on:

Sparse Encoder with Best Fields for efficient, accurate retrieval,
Adaptive retrieval decisions and minimal reflection tokens for dynamic, cost-effective grounding,
Simplified scaffolding and structured tutoring templates for pedagogical clarity with manageable complexity,
you achieve a resource-conscious, effective RAG system with intelligent tutoring capabilities that aligns with the core strengths of SELF-RAG, Blended RAG, and CLASS.

## Future Enhancements (Full BlendRAG Implementation)

The following features were part of the original BlendRAG design but have been temporarily removed from the data processing pipeline in `java8_rag.py` to significantly improve performance. They can be re-integrated in the future to create a more sophisticated and intelligent tutoring agent.

### 1. LLM-Generated Reasoning Steps
- **Description:** During the data-building phase, an LLM was used to generate detailed, step-by-step reasoning for each entry in the knowledge base.
- **Purpose:** These reasoning steps were intended to provide the tutoring model with a deeper understanding of the material, allowing it to generate more nuanced and pedagogically sound explanations.
- **Status:** Temporarily removed. The `reasoning_steps` column has been removed from the `knowledge_base` database table.

### 2. SELF-RAG Style Reflection Tokens
- **Description:** The data processing pipeline also included a step to generate SELF-RAG reflection tokens (e.g., `ISREL`, `ISSUP`). These tokens assess the relevance and supportiveness of retrieved passages.
- **Purpose:** At inference time, these tokens would allow the model to critically evaluate the information it retrieves, improving the accuracy and reliability of its responses by filtering out irrelevant or unsupported context.
- **Status:** Temporarily removed. The `reflection_tokens` column has been removed from the `knowledge_base` database table.

### 3. Advanced Inference-Time Pipeline
- **Description:** The full BlendRAG pipeline would utilize the `reasoning_steps` and `reflection_tokens` at inference time to make more intelligent decisions about when to retrieve information and how to use it.
- **Purpose:** This would enable a more dynamic and adaptive RAG system that can perform on-demand retrieval and self-correction, leading to higher-quality interactions with the user.
- **Status:** The current implementation uses a simplified RAG model. The advanced pipeline can be implemented once the data processing for the required features is re-enabled.
